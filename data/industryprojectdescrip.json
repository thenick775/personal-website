{
	"Airflow Ingestion Cluster":"This environment consists of a distributed Apache Airflow Cluster, backed by Celery, that utilizes an array of custom docker containers to fetch and process data from public webpages and data buckets, and send this processed data to the Ingestion API.\n\nDocker was used to isolate browser reliant code, and build reusable, language agnostic, one-time use containers. The containers are designed to allow for fast and reliable setup, deployment, testing, tear down, cloning, and failure recovery.",
	"Real-Time Ingestion API":"The Ingestion API is designed to function as the middle man between independent user programs orchestrated by Apache Airflow that collect data, and multiple postgres database connections. This API was built with golang, and utilizes GORM and Gorilla Mux for database operations and routing, and can concurrently process multipart forms in bulk for data ingestion through http requests.",
	"Ingestion Index Crawler":"This crawler is designed to reduce search times of already ingested data for an in-house manual data ingestion system. It crawls all the pages of existing instances for a specified user, and produces a list with the name, page number, and status of each instance. This allows for fast searching and navigation using the built in utility script. The crawler was built using Selenium in python and encapsulated in a docker container for easy deployment."
}